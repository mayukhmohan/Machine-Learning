setwd("E:/Machine Learning A-Z/Part 6 - Reinforcement Learning/Section 32 - Upper Confidence Bound (UCB)")
dataset = read.csv('Ads_CTR_Optimisation.csv')
View(dataset)
# Random Selection
# Importing the dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implementing Random Selection
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Random Selection
# Importing the dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implementing Random Selection
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Random Selection
# Importing the dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implementing Random Selection
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
# Random Selection
# Importing the dataset
dataset = read.csv('Ads_CTR_Optimisation.csv')
# Implementing Random Selection
N = 10000
d = 10
ads_selected = integer(0)
total_reward = 0
for (n in 1:N) {
ad = sample(1:10, 1)
ads_selected = append(ads_selected, ad)
reward = dataset[n, ad]
total_reward = total_reward + reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
dataset = read.csv('Ads_CTR_Optimisation.csv')
dataset = read.csv('Ads_CTR_Optimisation.csv')
#Implementing UCB
# Step 1
N = 10000
d = 10
ads_selected = integer()
numbers_of_selections = integer(d)
sums_of_rewards = integer(d)
total_rewards=0
for (n in 1:N){
ad = 0
max_upper_bound = 0
for (i in 1:d){
if(numbers_of_selections[i] > 0){
average_reward = sums_of_rewards[i] / numbers_of_selections[i]
delta_i = sqrt(3/2 * log(n) / numbers_of_selections[i])
upper_bound = average_reward + delta_i
}else{
upper_bound = 1e400 # For first ten observations each one gets selected
}
if(upper_bound > max_upper_bound){
max_upper_bound = upper_bound
ad = i
}
}
ads_selected = append(ads_selected,ad)
numbers_of_selections[ad] = numbers_of_selections[ad]+ 1
reward = dataset[n,ad]
sums_of_rewards[ad] = sums_of_rewards[ad] + reward
total_rewards=total_rewards+reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
ads_selected
options(max.print = 10000)
ads_selected
setwd("E:/Machine Learning A-Z/Part 6 - Reinforcement Learning/Section 33 - Thompson Sampling")
dataset = read.csv('Ads_CTR_Optimisation.csv')
dataset = read.csv('Ads_CTR_Optimisation.csv')
#Implementing Thompson Sampling
# Step 1
N = 10000
d = 10
ads_selected = integer()
numbers_of_rewards_1 = integer(d)
numbers_of_rewards_0 = integer(d)
total_rewards=0
for (n in 1:N){
ad = 0
max_random = 0
for (i in 1:d){
random_beta = rbeta(n=1, # number of random draw is one.
shape1=numbers_of_rewards_1[i]+1,
shape2=numbers_of_rewards_0[i]+1)
if(random_beta > max_random){
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
if(reward == 1){
numbers_of_rewards_1[ad]=numbers_of_rewards_1[ad]+1
}else{
numbers_of_rewards_0[ad]=numbers_of_rewards_0[ad]+1
}
total_rewards=total_rewards+reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
dataset = read.csv('Ads_CTR_Optimisation.csv')
#Implementing Thompson Sampling
# Step 1
N = 10000
d = 10
ads_selected = integer()
numbers_of_rewards_1 = integer(d)
numbers_of_rewards_0 = integer(d)
total_rewards=0
for (n in 1:N){
ad = 0
max_random = 0
for (i in 1:d){
random_beta = rbeta(n=1, # number of random draw is one.
shape1=numbers_of_rewards_1[i]+1,
shape2=numbers_of_rewards_0[i]+1)
if(random_beta > max_random){
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
if(reward == 1){
numbers_of_rewards_1[ad]=numbers_of_rewards_1[ad]+1
}else{
numbers_of_rewards_0[ad]=numbers_of_rewards_0[ad]+1
}
total_rewards=total_rewards+reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
dataset = read.csv('Ads_CTR_Optimisation.csv')
#Implementing Thompson Sampling
# Step 1
N = 10000
d = 10
ads_selected = integer()
numbers_of_rewards_1 = integer(d)
numbers_of_rewards_0 = integer(d)
total_rewards=0
for (n in 1:N){
ad = 0
max_random = 0
for (i in 1:d){
random_beta = rbeta(n=1, # number of random draw is one.
shape1=numbers_of_rewards_1[i]+1,
shape2=numbers_of_rewards_0[i]+1)
if(random_beta > max_random){
max_random = random_beta
ad = i
}
}
ads_selected = append(ads_selected,ad)
reward = dataset[n,ad]
if(reward == 1){
numbers_of_rewards_1[ad]=numbers_of_rewards_1[ad]+1
}else{
numbers_of_rewards_0[ad]=numbers_of_rewards_0[ad]+1
}
total_rewards=total_rewards+reward
}
# Visualising the results
hist(ads_selected,
col = 'blue',
main = 'Histogram of ads selections',
xlab = 'Ads',
ylab = 'Number of times each ad was selected')
setwd("E:/Machine Learning A-Z/Part 7 - Natural Language Processing/Section 36 - Natural Language Processing")
dataset = read.delim('Restaurant_Reviews.tsv',quote = '',stringsAsFactors = FALSE)
View(dataset)
install.packages('tm')
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
as.character(corpus[[1]])
corpus = tm_map(corpus,content_transformer(tolower))
as.character(corpus[[1]])
corpus = tm_map(corpus,removeNumbers)
as.character(corpus[[841]])
corpus = tm_map(corpus,removePunctuation)
as.character(corpus[[1]])
install.packages('SnowballC')
library(SnowballC)
corpus = tm_map(corpus,removeWords,stopwords())
as.character(corpus[[1]])
corpus = tm_map(corpus,stemDocument)
as.character(corpus[[1]])
corpus = tm_map(corpus,stripWhitespace)
as.character(corpus[[841]])
as.character(corpus[[1]])
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm,0.999)
dtm
dataset = as.data.frame(as.matrix(dtm))
View(dataset)
dataset_original = read.delim('Restaurant_Reviews.tsv',quote = '',stringsAsFactors = FALSE)
dataset$Liked = dataset_original$Liked
View(dataset)
#Random Forest Classification
#Encoding the target feature as factor
dataset$Liked=factor(dataset$Liked,levels = c(0,1))
#Splitting the dataset into training and test set
library(caTools)
set.seed(123)
split=sample.split(dataset$Liked,SplitRatio = 0.80)
training_set=subset(dataset,split==TRUE)
test_set=subset(dataset,split==FALSE)
#Fitting Random Forest Classification to the Training Set
#install.packages('randomForest')
library(randomForest)
classifier=randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 10)
#Predicting the Test set results
y_pred=predict(classifier, newdata = test_set[-692])
# Making the confusion metrics
cm = table(test_set[,692],y_pred)
cm
79+70
149/200
